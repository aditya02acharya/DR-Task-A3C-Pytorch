{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import math\n",
    "import h5py\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from skimage.transform import pyramid_gaussian, resize\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import multiprocessing as mp\n",
    "import threading\n",
    "\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='ACER')\n",
    "parser.add_argument('--seed', type=int, default=123, help='Random seed')\n",
    "parser.add_argument('--num-processes', type=int, default=3, metavar='N', help='Number of training async agents (does not include single validation agent)')\n",
    "parser.add_argument('--T-max', type=int, default=100000000, metavar='STEPS', help='Number of training steps')\n",
    "parser.add_argument('--t-max', type=int, default=100, metavar='STEPS', help='Max number of forward steps for A3C before update')\n",
    "parser.add_argument('--max-episode-length', type=int, default=15, metavar='LENGTH', help='Maximum episode length')\n",
    "parser.add_argument('--hidden-size', type=int, default=49, metavar='SIZE', help='Hidden size of LSTM cell')\n",
    "parser.add_argument('--n-dr-elements', type=int, default=49, metavar='SIZE', help='Number of objects in display')\n",
    "parser.add_argument('--present-action', type=int, default=49, metavar='SIZE', help='Present Action Value')\n",
    "parser.add_argument('--absent-action', type=int, default=50, metavar='SIZE', help='Absent Action Value')\n",
    "parser.add_argument('--memory-capacity', type=int, default=100000, metavar='CAPACITY', help='Experience replay memory capacity')\n",
    "parser.add_argument('--replay-ratio', type=int, default=4, metavar='r', help='Ratio of off-policy to on-policy updates')\n",
    "parser.add_argument('--replay-start', type=int, default=20000, metavar='EPISODES', help='Number of transitions to save before starting off-policy training')\n",
    "parser.add_argument('--discount', type=float, default=0.99, metavar='γ', help='Discount factor')\n",
    "parser.add_argument('--trace-decay', type=float, default=1, metavar='λ', help='Eligibility trace decay factor')\n",
    "parser.add_argument('--trace-max', type=float, default=10, metavar='c', help='Importance weight truncation (max) value')\n",
    "parser.add_argument('--trust-region-decay', type=float, default=0.99, metavar='α', help='Average model weight decay rate')\n",
    "parser.add_argument('--trust-region-threshold', type=float, default=1, metavar='δ', help='Trust region threshold value')\n",
    "parser.add_argument('--lr', type=float, default=0.0001, metavar='η', help='Learning rate')\n",
    "parser.add_argument('--rmsprop-decay', type=float, default=0.99, metavar='α', help='RMSprop decay factor')\n",
    "parser.add_argument('--batch-size', type=int, default=8, metavar='SIZE', help='Off-policy batch size')\n",
    "parser.add_argument('--entropy-weight', type=float, default=0.0001, metavar='β', help='Entropy regularisation weight')\n",
    "parser.add_argument('--max-gradient-norm', type=float, default=40, metavar='VALUE', help='Gradient L2 normalisation')\n",
    "parser.add_argument('--evaluation-interval', type=int, default=25000, metavar='STEPS', help='Number of training steps between evaluations (roughly)')\n",
    "parser.add_argument('--evaluation-episodes', type=int, default=10, metavar='N', help='Number of evaluation episodes to average over')\n",
    "parser.add_argument('--name', type=str, default='results', help='Save folder')\n",
    "parser.add_argument('--on-policy', action='store_true', help='Use pure on-policy training (A3C)')\n",
    "parser.add_argument('--trust-region', action='store_true', help='Use trust region')\n",
    "parser.add_argument('--pretrain-model-available', action='store_true', help='Use pre trained model weights')\n",
    "\n",
    "N_DR_ELEMENTS = 49\n",
    "N_ACTIONS = N_DR_ELEMENTS + 2\n",
    "PRESENT = N_DR_ELEMENTS\n",
    "ABSENT = N_DR_ELEMENTS+1\n",
    "MAX_STEPS = 15\n",
    "N_ROWS = 7\n",
    "PATH=\"./Lab_Model\"\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "IMG_CHANNELs = 3\n",
    "\n",
    "PYRAMID_LEVEL = 4 #excluding the original image as a level.\n",
    "\n",
    "CONVERSION_FACTOR = 6/15.5 #number of pixels per degree. Here, 15.5 is the display size in degree (size used in the experiment).\n",
    "\n",
    "FOVEA = 2 #in degrees\n",
    "\n",
    "FOVEA_RADIUS = int(np.round(FOVEA/CONVERSION_FACTOR)) #In pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Counter():\n",
    "    def __init__(self):\n",
    "        self.val = mp.Value('i', 0)\n",
    "        self.lock = mp.Lock()\n",
    "    \n",
    "    def increment(self):\n",
    "        with self.lock:\n",
    "            self.val.value += 1\n",
    "\n",
    "    def value(self):\n",
    "        with self.lock:\n",
    "            return self.val.value\n",
    "\n",
    "def state_to_tensor(state):\n",
    "    return torch.from_numpy(state).float().unsqueeze(0)\n",
    "\n",
    "#setup mapping of fixated location to its corresponding x,y coordinate in the image.\n",
    "fixated_location = 0\n",
    "FIXATION_DICT = {}\n",
    "hor = 5\n",
    "ver = 5\n",
    "for row in range(7):\n",
    "    for col in range(7):\n",
    "        FIXATION_DICT[str(fixated_location)] = (hor, ver)\n",
    "        fixated_location += 1\n",
    "        hor = hor +  9\n",
    "    ver = ver +  9\n",
    "    hor = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'policy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodicReplayMemory():\n",
    "    def __init__(self, capacity, max_episode_length):\n",
    "        # Max number of transitions possible will be the memory capacity, could be much less\n",
    "        self.num_episodes = capacity // max_episode_length\n",
    "        self.memory = deque(maxlen=self.num_episodes)\n",
    "        self.trajectory = []\n",
    "\n",
    "    def append(self, state, action, reward, policy):\n",
    "        self.trajectory.append(Transition(state, action, reward, policy))  # Save s_i, a_i, r_i+1, µ(·|s_i)\n",
    "        # Terminal states are saved with actions as None, so switch to next episode\n",
    "        if action is None:\n",
    "            self.memory.append(self.trajectory)\n",
    "            self.trajectory = []\n",
    "        \n",
    "    # Samples random trajectory\n",
    "    def sample(self, maxlen=0):\n",
    "        mem = self.memory[random.randrange(len(self.memory))]\n",
    "        T = len(mem)\n",
    "        # Take a random subset of trajectory if maxlen specified, otherwise return full trajectory\n",
    "        if maxlen > 0 and T > maxlen + 1:\n",
    "            t = random.randrange(T - maxlen - 1)  # Include next state after final \"maxlen\" state\n",
    "            return mem[t:t + maxlen + 1]\n",
    "        else:\n",
    "            return mem\n",
    "    \n",
    "    # Samples batch of trajectories, truncating them to the same length\n",
    "    def sample_batch(self, batch_size, maxlen=0):\n",
    "        batch = [self.sample(maxlen=maxlen) for _ in range(batch_size)]\n",
    "        minimum_size = min(len(trajectory) for trajectory in batch)\n",
    "        batch = [trajectory[:minimum_size] for trajectory in batch]  # Truncate trajectories\n",
    "        return list(map(list, zip(*batch)))  # Transpose so that timesteps are packed together\n",
    "\n",
    "    def length(self):\n",
    "        # Return number of epsiodes saved in memory\n",
    "        return len(self.memory)\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(episode) for episode in self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedRMSprop(optim.RMSprop):\n",
    "    def __init__(self, params, lr=1e-2, alpha=0.99, eps=1e-8, weight_decay=0):\n",
    "        super(SharedRMSprop, self).__init__(params, lr=lr, alpha=alpha, eps=eps, weight_decay=weight_decay, momentum=0, centered=False)\n",
    "\n",
    "        # State initialisation (must be done before step, else will not be shared between threads)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = p.data.new().resize_(1).zero_()\n",
    "                state['square_avg'] = p.data.new().resize_as_(p.data).zero_()\n",
    "\n",
    "    def share_memory(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'].share_memory_()\n",
    "                state['square_avg'].share_memory_()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(group['weight_decay'], p.data)\n",
    "                    \n",
    "                # g = αg + (1 - α)Δθ^2\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n",
    "                \n",
    "                # θ ← θ - ηΔθ/√(g + ε)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "                p.data.addcdiv_(-group['lr'], grad, avg)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unit(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels, padding):\n",
    "        super(Unit,self).__init__()\n",
    "        \n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels,kernel_size=3,out_channels=out_channels,stride=1,padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,input):\n",
    "        output = self.conv(input)\n",
    "        output = self.bn(output)\n",
    "        output = self.relu(output)\n",
    "        return output\n",
    "\n",
    "class CNN_Module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Module, self).__init__()\n",
    "        \n",
    "        self.unit1 = Unit(in_channels=3,out_channels=8, padding=0)\n",
    "        self.unit2 = Unit(in_channels=8,out_channels=16, padding=0)\n",
    "        self.unit3 = Unit(in_channels=16,out_channels=24, padding=0)\n",
    "        self.unit4 = Unit(in_channels=24,out_channels=32, padding=0)\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.unit5 = Unit(in_channels=32,out_channels=32, padding=1)\n",
    "        self.unit6 = Unit(in_channels=32,out_channels=32, padding=1)\n",
    "        self.unit7 = Unit(in_channels=32,out_channels=32, padding=1)\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.unit8 = Unit(in_channels=32,out_channels=32, padding=1)\n",
    "        self.unit9 = Unit(in_channels=32,out_channels=32, padding=1)\n",
    "        \n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.unit10 = Unit(in_channels=32,out_channels=32, padding=1)\n",
    "        \n",
    "        self.net = nn.Sequential(self.unit1, self.unit2, self.unit3, self.unit4, self.pool1, self.unit5, self.unit6, \\\n",
    "                                 self.unit7, self.pool2, self.unit8, self.unit9, self.pool3,  self.unit10)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=7 * 7 * 32, out_features=392)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(in_features=392, out_features=98)\n",
    "        self.activation2 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(-1, 3, 64, 64)\n",
    "        output = self.net(input)\n",
    "        output = output.view(-1, 7 * 7 * 32)\n",
    "        output = self.fc1(output)\n",
    "        output = self.activation1(output)\n",
    "        output = self.fc2(output)\n",
    "        output = self.activation2(output)\n",
    "        return output\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        #self.cnn = CNN_Module()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.lstm = nn.LSTMCell(hidden_size, hidden_size)\n",
    "        self.fc_actor = nn.Linear(hidden_size, action_size)\n",
    "        self.fc_critic = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, x, h):   \n",
    "        x = self.fc1(x)\n",
    "        h = self.lstm(x, h)  # h is (hidden state, cell state)\n",
    "        x = h[0]\n",
    "        policy = F.softmax(self.fc_actor(x), dim=1).clamp(max=1 - 1e-20)  # Prevent 1s and hence NaNs\n",
    "        Q = self.fc_critic(x)\n",
    "        V = (Q * policy).sum(1, keepdim=True)  # V is expectation of Q under π\n",
    "        return policy, Q, V, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self, args):\n",
    "        self.num_feats = N_DR_ELEMENTS\n",
    "        self.num_actions = N_ACTIONS\n",
    "        self.steps = 0\n",
    "        self.total_time = 0.0\n",
    "        self.image = None\n",
    "        self.state = None\n",
    "        self.model = CNN_Module()\n",
    "        self.model.load_state_dict(torch.load(PATH, map_location='cpu'))\n",
    "        #self.model.to(torch.device('cpu'))\n",
    "        self.correct = 0\n",
    "        self.target_present = False\n",
    "        \n",
    "        path = os.path.join('.','dr_data.h5')\n",
    "        self.dr_data = h5py.File(path, 'r')\n",
    "        \n",
    "        \n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        self.steps += 1\n",
    "        done = False\n",
    "        reward = -0.1\n",
    "        info = ''\n",
    "        \n",
    "        if action < N_DR_ELEMENTS:\n",
    "            fixation_loc = FIXATION_DICT[str(action)]\n",
    "            fixate_x = int(action / N_ROWS)\n",
    "            fixate_y = int(action % N_ROWS)\n",
    "            input_image = self.sampling(self.image, fixation_loc[0], fixation_loc[1])\n",
    "            with torch.no_grad():\n",
    "                self.prob_out = self.model(torch.from_numpy(input_image).float().to(torch.device('cpu')))\n",
    "            self.state = self.prob_out[0].detach().numpy()\n",
    "        \n",
    "        elif (action == PRESENT and self.target_present) or (action == ABSENT and not self.target_present):\n",
    "            reward = 2.0\n",
    "            done = True\n",
    "            self.correct = 1\n",
    "        \n",
    "        else:\n",
    "            reward = -2.0\n",
    "            done = True\n",
    "            self.correct = 0\n",
    "        \n",
    "        if self.steps >= MAX_STEPS:\n",
    "            done = True\n",
    "            self.correct = 0\n",
    "\n",
    "        \n",
    "        return self.state.flatten(), reward, done, info\n",
    "            \n",
    "    def reset(self):\n",
    "        idx = np.random.randint(len(self.dr_data[\"Images\"]))\n",
    "        self.image = self.dr_data[\"Images\"][idx]\n",
    "        self.image = self.image.astype('uint8')\n",
    "        \n",
    "        self.steps = 0\n",
    "        self.total_time = 0.0\n",
    "        \n",
    "        self.correct = 0\n",
    "        self.target_present = True if self.dr_data[\"target_status\"][idx] == 1 else False\n",
    "        \n",
    "        self.state = np.zeros((1, N_DR_ELEMENTS+N_DR_ELEMENTS))\n",
    "        \n",
    "        return self.state.flatten()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_eccentricity(self, fixated_x, fixated_y):\n",
    "        #Generate a mask with shape similar to the image.\n",
    "        mask = 255*np.ones((IMG_HEIGHT, IMG_WIDTH), dtype='uint8')\n",
    "    \n",
    "        #Fovea is represented as a circle at fixated_x, fixated_y of radius FOVEA_RADIUS.\n",
    "        cv2.circle(mask, (fixated_x, fixated_y), FOVEA_RADIUS, 0, -1)\n",
    "    \n",
    "        #Apply distance transform to mask. Open cv implementation of ecludian distance from fovea.\n",
    "        eccentricity = cv2.distanceTransform(mask, cv2.DIST_L2, 3)\n",
    "        eccentricity = eccentricity/CONVERSION_FACTOR\n",
    "        eccentricity = (eccentricity / np.max(eccentricity)) * PYRAMID_LEVEL\n",
    "        eccentricity = np.round(eccentricity)\n",
    "        eccentricity = eccentricity.astype(np.int)\n",
    "    \n",
    "        return eccentricity\n",
    "\n",
    "    def smooth_pyramid(self, image, layers=4):\n",
    "        pyr_img = []\n",
    "        for (i, resized) in enumerate(pyramid_gaussian(image, max_layer=layers, downscale=1.7, multichannel=True)):\n",
    "            pyr_img.append(resize(resized, (64,64), anti_aliasing=False, preserve_range=True, anti_aliasing_sigma=i**4, mode='constant'))\n",
    "        return pyr_img\n",
    "\n",
    "\n",
    "    def sampling(self, image, fixate_x, fixate_y):\n",
    "        eccentricity = self.get_eccentricity(fixate_x, fixate_y)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2Lab)\n",
    "        pyramid = self.smooth_pyramid(image, PYRAMID_LEVEL)\n",
    "        im_ = np.zeros(image.shape)\n",
    "        for ecc in range(np.max(eccentricity)+1):\n",
    "            i  = np.argwhere(eccentricity == ecc)\n",
    "            if len(i) > 0:\n",
    "                im_[i[:,0], i[:,1]] = pyramid[ecc][i[:,0], i[:,1]]\n",
    "        im_ = im_.reshape(-1,64,64,3)\n",
    "        #Pytorch accepts images as [channel, width, height]\n",
    "        im_ = np.swapaxes(im_, 3, 2)\n",
    "        im_ = np.swapaxes(im_, 2, 1)\n",
    "        return im_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(rank, args, T, shared_model):\n",
    "    \n",
    "    torch.manual_seed(args.seed + rank)\n",
    "    \n",
    "    env = env = Env(args)\n",
    "    \n",
    "    model = ActorCritic(N_DR_ELEMENTS+N_DR_ELEMENTS, N_ACTIONS, args.hidden_size)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    save_dir = os.path.join('.', args.name)  \n",
    "    \n",
    "    can_test = True  # Test flag\n",
    "    \n",
    "    t_start = 1  # Test step counter to check against global counter\n",
    "    \n",
    "    rewards, accuracy, steps = [], [], []  # Rewards and steps for plotting\n",
    "    l = str(len(str(args.T_max)))  # Max num. of digits for logging steps\n",
    "    done = True  # Start new episode\n",
    "    \n",
    "    # stores step, reward, avg_steps and time \n",
    "    results_dict = {'t': [], 'reward': [], 'accuracy': [], 'avg_steps': [], 'time': []}\n",
    "    \n",
    "    while T.value() <= args.T_max:\n",
    "        if can_test:\n",
    "            t_start = T.value()  # Reset counter\n",
    "            \n",
    "            # Evaluate over several episodes and average results\n",
    "            avg_rewards, avg_episode_lengths, avg_accuracy = [], [], []\n",
    "            \n",
    "            for _ in range(args.evaluation_episodes):\n",
    "                while True:\n",
    "                    # Reset or pass on hidden state\n",
    "                    if done:\n",
    "                        # Sync with shared model every episode\n",
    "                        model.load_state_dict(shared_model.state_dict())\n",
    "                        hx = torch.zeros(1, args.hidden_size)\n",
    "                        cx = torch.zeros(1, args.hidden_size)\n",
    "                        \n",
    "                        # Reset environment and done flag\n",
    "                        state = state_to_tensor(env.reset())\n",
    "                        done, episode_length = False, 0\n",
    "                        reward_sum = 0\n",
    "                    \n",
    "                    # Calculate policy\n",
    "                    with torch.no_grad():\n",
    "                        policy, _, _, (hx, cx) = model(state, (hx, cx))\n",
    "                    \n",
    "                    # Choose action greedily\n",
    "                    action = policy.max(1)[1][0]\n",
    "                    \n",
    "                    # Step\n",
    "                    state, reward, done, _ = env.step(action.item())\n",
    "                    \n",
    "                    state = state_to_tensor(state)\n",
    "                    reward_sum += reward\n",
    "                    episode_length += 1  # Increase episode counter\n",
    "                    \n",
    "                    # Log and reset statistics at the end of every episode\n",
    "                    if done:\n",
    "                        avg_rewards.append(reward_sum)\n",
    "                        avg_episode_lengths.append(episode_length)\n",
    "                        avg_accuracy.append(env.correct)\n",
    "                        break\n",
    "\n",
    "            print(('[{}] Step: {:<' + l + '} Avg. Reward: {:<8} Avg. Episode Length: {:<8} Avg. Accuracy: {:<8}').format(\n",
    "                datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S,%f')[:-3],t_start,\n",
    "                sum(avg_rewards) / args.evaluation_episodes,sum(avg_episode_lengths) / args.evaluation_episodes,sum(avg_accuracy) / args.evaluation_episodes))\n",
    "            \n",
    "            fields = [t_start, sum(avg_rewards) / args.evaluation_episodes, sum(avg_episode_lengths) / args.evaluation_episodes, sum(accuracy) / args.evaluation_episodes, str(datetime.now())]\n",
    "            \n",
    "            # storing data in the dictionary.\n",
    "            results_dict['t'].append(t_start)\n",
    "            results_dict['reward'].append(sum(avg_rewards) / args.evaluation_episodes)\n",
    "            results_dict['avg_steps'].append(sum(avg_episode_lengths) / args.evaluation_episodes)\n",
    "            results_dict['time'].append(str(datetime.now()))\n",
    "            results_dict['accuracy'].append(sum(avg_accuracy) / args.evaluation_episodes)\n",
    "            \n",
    "            # Dumping the results in pickle format  \n",
    "            with open(os.path.join(save_dir, 'results.pck'), 'wb') as f:\n",
    "                pickle.dump(results_dict, f)\n",
    "            \n",
    "            # Saving the data in csv format\n",
    "            with open(os.path.join(save_dir, 'test_results.csv'), 'a') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(fields)\n",
    "            \n",
    "            \n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, 'model.pth'))  # Save model params\n",
    "            can_test = False  # Finish testing\n",
    "        else:\n",
    "            if T.value() - t_start >= args.evaluation_interval:\n",
    "                can_test = True\n",
    "\n",
    "\n",
    "        time.sleep(0.001)  # Check if available to test every millisecond\n",
    "\n",
    "    # Dumping the results in pickle format  \n",
    "    with open(os.path.join(save_dir, 'results.pck'), 'wb') as f:\n",
    "        pickle.dump(results_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knuth's algorithm for generating Poisson samples\n",
    "def _poisson(lmbd):\n",
    "    L, k, p = math.exp(-lmbd), 0, 1\n",
    "    while p > L:\n",
    "        k += 1\n",
    "        p *= random.uniform(0, 1)\n",
    "    return max(k - 1, 0)\n",
    "\n",
    "# Transfers gradients from thread-specific model to shared model\n",
    "def _transfer_grads_to_shared_model(model, shared_model):\n",
    "    for param, shared_param in zip(model.parameters(), shared_model.parameters()):\n",
    "        if shared_param.grad is not None:\n",
    "            return\n",
    "        shared_param._grad = param.grad\n",
    "\n",
    "\n",
    "# Adjusts learning rate\n",
    "def _adjust_learning_rate(optimiser, lr):\n",
    "    for param_group in optimiser.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "\n",
    "# Updates networks\n",
    "def _update_networks(args, T, model, shared_model, shared_average_model, loss, optimiser):\n",
    "    # Zero shared and local grads\n",
    "    optimiser.zero_grad()\n",
    "    \"\"\"\n",
    "    Calculate gradients for gradient descent on loss functions\n",
    "    Note that math comments follow the paper, which is formulated for gradient ascent\n",
    "    \"\"\"\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient L2 normalisation\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), args.max_gradient_norm)\n",
    "    \n",
    "    # Transfer gradients to shared model and update\n",
    "    _transfer_grads_to_shared_model(model, shared_model)\n",
    "    optimiser.step()\n",
    "    \n",
    "    # Update shared_average_model\n",
    "    for shared_param, shared_average_param in zip(shared_model.parameters(), shared_average_model.parameters()):\n",
    "        shared_average_param = args.trust_region_decay * shared_average_param + (1 - args.trust_region_decay) * shared_param\n",
    "\n",
    "        \n",
    "# Computes an \"efficient trust region\" loss (policy head only) based on an existing loss and two distributions\n",
    "def _trust_region_loss(model, distribution, ref_distribution, loss, threshold, g, k):\n",
    "    \n",
    "    kl = - (ref_distribution * (distribution.log()-ref_distribution.log())).sum(1).mean(0)\n",
    "    \n",
    "    # Compute dot products of gradients\n",
    "    k_dot_g = (k*g).sum(1).mean(0)\n",
    "    k_dot_k = (k**2).sum(1).mean(0)\n",
    "    \n",
    "    # Compute trust region update\n",
    "    if k_dot_k.item() > 0:\n",
    "        trust_factor = ((k_dot_g - threshold) / k_dot_k).clamp(min=0).detach()\n",
    "    else:\n",
    "        trust_factor = torch.zeros(1)\n",
    "    \n",
    "    # z* = g - max(0, (k^T∙g - δ) / ||k||^2_2)∙k\n",
    "    trust_loss = loss + trust_factor*kl\n",
    "    \n",
    "    return trust_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains model\n",
    "def _train(args, T, model, shared_model, shared_average_model, optimiser, policies, Qs, Vs, actions, rewards, Qret, average_policies, old_policies=None):\n",
    "    off_policy = old_policies is not None\n",
    "    \n",
    "    action_size = policies[0].size(1)\n",
    "    \n",
    "    policy_loss, value_loss = 0, 0\n",
    "    \n",
    "    # Calculate n-step returns in forward view, stepping backwards from the last state\n",
    "    \n",
    "    t = len(rewards)\n",
    "    \n",
    "    for i in reversed(range(t)):        \n",
    "        # Importance sampling weights ρ ← π(∙|s_i) / µ(∙|s_i); 1 for on-policy\n",
    "        if off_policy:\n",
    "            rho = policies[i].detach() / old_policies[i]\n",
    "        else:\n",
    "            rho = torch.ones(1, action_size)\n",
    "            \n",
    "        # Qret ← r_i + γQret\n",
    "        Qret = rewards[i] + args.discount * Qret\n",
    "    \n",
    "        # Advantage A ← Qret - V(s_i; θ)\n",
    "        A = Qret - Vs[i]\n",
    "\n",
    "        # Log policy log(π(a_i|s_i; θ))\n",
    "        log_prob = policies[i].gather(1, actions[i]).log()\n",
    "    \n",
    "        # g ← min(c, ρ_a_i)∙∇θ∙log(π(a_i|s_i; θ))∙A\n",
    "        single_step_policy_loss = -(rho.gather(1, actions[i]).clamp(max=args.trace_max) * log_prob * A.detach()).mean(0)  # Average over batch\n",
    "    \n",
    "        # Off-policy bias correction\n",
    "        if off_policy:\n",
    "            # g ← g + Σ_a [1 - c/ρ_a]_+∙π(a|s_i; θ)∙∇θ∙log(π(a|s_i; θ))∙(Q(s_i, a; θ) - V(s_i; θ)\n",
    "            bias_weight = (1 - args.trace_max / rho).clamp(min=0) * policies[i]\n",
    "            single_step_policy_loss -= (bias_weight * policies[i].log() * (Qs[i].detach() - Vs[i].expand_as(Qs[i]).detach())).sum(1).mean(0)\n",
    "    \n",
    "        if args.trust_region:        \n",
    "            # KL divergence k ← ∇θ0∙DKL[π(∙|s_i; θ_a) || π(∙|s_i; θ)]\n",
    "            k = -average_policies[i].gather(1, actions[i]) / (policies[i].gather(1, actions[i]) + 1e-10)\n",
    "        \n",
    "            if off_policy:\n",
    "                g = (rho.gather(1, actions[i]).clamp(max=args.trace_max) * A / (policies[i] + 1e-10).gather(1, actions[i]) \\\n",
    "                     + (bias_weight * (Qs[i] - Vs[i].expand_as(Qs[i]))/(policies[i] + 1e-10)).sum(1)).detach()\n",
    "        \n",
    "            else:\n",
    "                g = (rho.gather(1, actions[i]).clamp(max=args.trace_max) * A / (policies[i] + 1e-10).gather(1, actions[i])).detach()\n",
    "      \n",
    "            # Policy update dθ ← dθ + ∂θ/∂θ∙z*\n",
    "            policy_loss += _trust_region_loss(model, policies[i].gather(1, actions[i]) + 1e-10, average_policies[i].gather(1, actions[i]) + 1e-10, single_step_policy_loss, args.trust_region_threshold, g, k)\n",
    "        \n",
    "        else:\n",
    "            # Policy update dθ ← dθ + ∂θ/∂θ∙g\n",
    "            policy_loss += single_step_policy_loss\n",
    "    \n",
    "        # Entropy regularisation dθ ← dθ + β∙∇θH(π(s_i; θ))\n",
    "        policy_loss -= args.entropy_weight * -(policies[i].log() * policies[i]).sum(1).mean(0)  # Sum over probabilities, average over batch\n",
    "\n",
    "        # Value update dθ ← dθ - ∇θ∙1/2∙(Qret - Q(s_i, a_i; θ))^2\n",
    "        Q = Qs[i].gather(1, actions[i])\n",
    "        value_loss += ((Qret - Q) ** 2 / 2).mean(0)  # Least squares loss\n",
    "\n",
    "        # Truncated importance weight ρ¯_a_i = min(1, ρ_a_i)\n",
    "        truncated_rho = rho.gather(1, actions[i]).clamp(max=1)\n",
    "        # Qret ← ρ¯_a_i∙(Qret - Q(s_i, a_i; θ)) + V(s_i; θ)\n",
    "        Qret = truncated_rho * (Qret - Q.detach()) + Vs[i].detach()\n",
    "    \n",
    "    # Update networks\n",
    "    _update_networks(args, T, model, shared_model, shared_average_model, policy_loss + value_loss, optimiser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acts and trains model\n",
    "def train(rank, args, T, shared_model, shared_average_model, optimiser):\n",
    "    torch.manual_seed(args.seed + rank)\n",
    "    \n",
    "    env = Env(args)\n",
    "    \n",
    "    model = ActorCritic(N_DR_ELEMENTS+N_DR_ELEMENTS, N_ACTIONS, args.hidden_size)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    if not args.on_policy:\n",
    "        # Normalise memory capacity by number of training processes\n",
    "        memory = EpisodicReplayMemory(args.memory_capacity // args.num_processes, args.max_episode_length)\n",
    "        \n",
    "    t = 1  # Thread step counter\n",
    "    done = True  # Start new episode\n",
    "        \n",
    "    while T.value() <= args.T_max:\n",
    "        # On-policy episode loop\n",
    "        \n",
    "        while True:\n",
    "            # Sync with shared model at least every t_max steps\n",
    "            model.load_state_dict(shared_model.state_dict())\n",
    "            # Get starting timestep\n",
    "            t_start = t\n",
    "            \n",
    "            # Reset or pass on hidden state\n",
    "            if done:\n",
    "                hx, avg_hx = torch.zeros(1, args.hidden_size), torch.zeros(1, args.hidden_size)\n",
    "                cx, avg_cx = torch.zeros(1, args.hidden_size), torch.zeros(1, args.hidden_size)\n",
    "                \n",
    "                # Reset environment and done flag\n",
    "                state = state_to_tensor(env.reset())\n",
    "                done, episode_length, prev_action = False, 0, -1\n",
    "            else:\n",
    "                # Perform truncated backpropagation-through-time (allows freeing buffers after backwards call)\n",
    "                hx = hx.detach()\n",
    "                cx = cx.detach()\n",
    "            \n",
    "            # Lists of outputs for training\n",
    "            policies, Qs, Vs, actions, rewards, average_policies = [], [], [], [], [], []\n",
    "            \n",
    "            while not done and t - t_start < args.t_max:\n",
    "                # Calculate policy and values\n",
    "                policy, Q, V, (hx, cx) = model(state, (hx, cx))\n",
    "                average_policy, _, _, (avg_hx, avg_cx) = shared_average_model(state, (avg_hx, avg_cx))\n",
    "\n",
    "                # Sample action\n",
    "                action = torch.multinomial(policy, 1)[0, 0]\n",
    "\n",
    "                # Step\n",
    "                next_state, reward, done, _ = env.step(action.item())\n",
    "                next_state = state_to_tensor(next_state)\n",
    "                episode_length += 1  # Increase episode counter\n",
    "\n",
    "                if not args.on_policy:\n",
    "                    # Save (beginning part of) transition for offline training\n",
    "                    memory.append(state, action, reward, policy.detach())  # Save just tensors\n",
    "                \n",
    "                # Save outputs for online training\n",
    "                [arr.append(el) for arr, el in zip((policies, Qs, Vs, actions, rewards, average_policies),\n",
    "                                (policy, Q, V, torch.LongTensor([[action]]), torch.Tensor([[reward]]), average_policy))]\n",
    "                \n",
    "                # Increment counters\n",
    "                t += 1\n",
    "                T.increment()\n",
    "                \n",
    "                # Update state\n",
    "                state = next_state\n",
    "                prev_action = action\n",
    "            \n",
    "            # Break graph for last values calculated (used for targets, not directly as model outputs)\n",
    "            \n",
    "            if done:\n",
    "                # Qret = 0 for terminal s\n",
    "                if prev_action == PRESENT or prev_action == ABSENT:\n",
    "                    Qret = torch.zeros(1, 1)\n",
    "                else:\n",
    "                    _, _, Qret, _ = model(state, (hx, cx))\n",
    "                    Qret = Qret.detach()\n",
    "                        \n",
    "                \n",
    "                if not args.on_policy:\n",
    "                    # Save terminal state for offline training\n",
    "                    memory.append(state, None, None, None)\n",
    "            \n",
    "            else:\n",
    "                # Qret = V(s_i; θ) for non-terminal s\n",
    "                _, _, Qret, _ = model(state, (hx, cx))\n",
    "                Qret = Qret.detach()\n",
    "                \n",
    "            # Train the network on-policy\n",
    "            _train(args, T, model, shared_model, shared_average_model, optimiser, policies, Qs, Vs, actions, rewards, Qret, average_policies)\n",
    "            \n",
    "            # Finish on-policy episode\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Train the network off-policy when enough experience has been collected\n",
    "        if not args.on_policy and len(memory) >= args.replay_start:\n",
    "            # Sample a number of off-policy episodes based on the replay ratio\n",
    "            for _ in range(_poisson(args.replay_ratio)):\n",
    "                # Act and train off-policy for a batch of (truncated) episode\n",
    "                trajectories = memory.sample_batch(args.batch_size, maxlen=args.t_max)\n",
    "                \n",
    "                # Reset hidden state\n",
    "                hx, avg_hx = torch.zeros(args.batch_size, args.hidden_size), torch.zeros(args.batch_size, args.hidden_size)\n",
    "                cx, avg_cx = torch.zeros(args.batch_size, args.hidden_size), torch.zeros(args.batch_size, args.hidden_size)\n",
    "                \n",
    "                # Lists of outputs for training\n",
    "                policies, Qs, Vs, actions, rewards, old_policies, average_policies = [], [], [], [], [], [], []\n",
    "                \n",
    "                # Loop over trajectories (bar last timestep)\n",
    "                for i in range(len(trajectories) - 1):\n",
    "                    # Unpack first half of transition\n",
    "                    state = torch.cat(tuple(trajectory.state for trajectory in trajectories[i]), 0)\n",
    "                    action = torch.LongTensor([trajectory.action for trajectory in trajectories[i]]).unsqueeze(1)\n",
    "                    reward = torch.Tensor([trajectory.reward for trajectory in trajectories[i]]).unsqueeze(1)\n",
    "                    old_policy = torch.cat(tuple(trajectory.policy for trajectory in trajectories[i]), 0)\n",
    "                    \n",
    "                    # Calculate policy and values\n",
    "                    policy, Q, V, (hx, cx) = model(state, (hx, cx))\n",
    "                    average_policy, _, _, (avg_hx, avg_cx) = shared_average_model(state, (avg_hx, avg_cx))\n",
    "                    \n",
    "                    # Save outputs for offline training\n",
    "                    [arr.append(el) for arr, el in zip((policies, Qs, Vs, actions, rewards, average_policies, old_policies),\n",
    "                                             (policy, Q, V, action, reward, average_policy, old_policy))]\n",
    "                    \n",
    "                    # Unpack second half of transition\n",
    "                    next_state = torch.cat(tuple(trajectory.state for trajectory in trajectories[i + 1]), 0)\n",
    "                    done = torch.Tensor([trajectory.action is None for trajectory in trajectories[i + 1]]).unsqueeze(1)\n",
    "                \n",
    "                # Do forward pass for all transitions\n",
    "                _, _, Qret, _ = model(next_state, (hx, cx))\n",
    "                \n",
    "                # Qret = 0 for terminal s, V(s_i; θ) otherwise\n",
    "                Qret = ((1 - done) * Qret).detach()\n",
    "                \n",
    "                # Train the network off-policy\n",
    "                _train(args, T, model, shared_model, shared_average_model, optimiser, policies, Qs, Vs,\n",
    "                       actions, rewards, Qret, average_policies, old_policies=old_policies)\n",
    "        done = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 1 started\n",
      "Process 2 started\n",
      "Process 3 started\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "[2019-04-06 09:42:12,797] Step: 0         Avg. Reward: -1.5000000000000002 Avg. Episode Length: 15.0     Avg. Accuracy: 0.0     \n",
      "[2019-04-06 09:52:52,782] Step: 25001     Avg. Reward: -0.8     Avg. Episode Length: 1.0      Avg. Accuracy: 0.3     \n",
      "[2019-04-06 10:06:04,806] Step: 50002     Avg. Reward: -1.2     Avg. Episode Length: 1.0      Avg. Accuracy: 0.2     \n",
      "[2019-04-06 10:21:38,001] Step: 75002     Avg. Reward: -0.4     Avg. Episode Length: 1.0      Avg. Accuracy: 0.4     \n",
      "[2019-04-06 10:37:40,011] Step: 100003    Avg. Reward: -0.8     Avg. Episode Length: 1.0      Avg. Accuracy: 0.3     \n",
      "[2019-04-06 10:53:39,885] Step: 125003    Avg. Reward: 0.4      Avg. Episode Length: 1.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-06 11:09:34,889] Step: 150004    Avg. Reward: 0.4      Avg. Episode Length: 1.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-06 11:25:48,447] Step: 175006    Avg. Reward: 0.4      Avg. Episode Length: 1.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-06 11:41:53,780] Step: 200007    Avg. Reward: 0.0      Avg. Episode Length: 1.0      Avg. Accuracy: 0.5     \n",
      "[2019-04-06 11:57:55,006] Step: 225008    Avg. Reward: 0.4      Avg. Episode Length: 1.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-06 12:13:52,612] Step: 250008    Avg. Reward: -0.8     Avg. Episode Length: 1.0      Avg. Accuracy: 0.3     \n",
      "[2019-04-06 12:29:59,892] Step: 275009    Avg. Reward: -0.4     Avg. Episode Length: 1.0      Avg. Accuracy: 0.4     \n",
      "[2019-04-06 12:45:48,872] Step: 300010    Avg. Reward: -0.4     Avg. Episode Length: 1.0      Avg. Accuracy: 0.4     \n",
      "[2019-04-06 13:01:27,003] Step: 325010    Avg. Reward: 0.4      Avg. Episode Length: 1.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-06 13:15:49,165] Step: 350010    Avg. Reward: 0.0      Avg. Episode Length: 1.0      Avg. Accuracy: 0.5     \n",
      "[2019-04-06 13:31:23,859] Step: 375011    Avg. Reward: 1.2      Avg. Episode Length: 1.0      Avg. Accuracy: 0.8     \n",
      "[2019-04-06 13:46:47,843] Step: 400011    Avg. Reward: -0.4     Avg. Episode Length: 1.0      Avg. Accuracy: 0.4     \n",
      "[2019-04-06 14:02:06,729] Step: 425011    Avg. Reward: 0.0      Avg. Episode Length: 1.0      Avg. Accuracy: 0.5     \n",
      "[2019-04-06 14:16:53,581] Step: 450011    Avg. Reward: -0.4     Avg. Episode Length: 1.0      Avg. Accuracy: 0.4     \n",
      "[2019-04-06 14:30:40,774] Step: 475012    Avg. Reward: 0.0      Avg. Episode Length: 1.0      Avg. Accuracy: 0.5     \n",
      "[2019-04-06 14:44:19,982] Step: 500012    Avg. Reward: 0.4      Avg. Episode Length: 1.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-06 14:58:03,692] Step: 525013    Avg. Reward: 0.4      Avg. Episode Length: 1.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-06 15:11:45,288] Step: 550015    Avg. Reward: 0.4      Avg. Episode Length: 1.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-06 15:25:38,866] Step: 575017    Avg. Reward: -1.2     Avg. Episode Length: 1.0      Avg. Accuracy: 0.2     \n",
      "[2019-04-06 15:39:57,752] Step: 600017    Avg. Reward: 0.4      Avg. Episode Length: 1.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-06 15:54:33,768] Step: 625017    Avg. Reward: -0.8     Avg. Episode Length: 1.0      Avg. Accuracy: 0.3     \n",
      "[2019-04-06 16:09:07,219] Step: 650017    Avg. Reward: -1.2     Avg. Episode Length: 1.0      Avg. Accuracy: 0.2     \n",
      "[2019-04-06 16:23:27,248] Step: 675018    Avg. Reward: 0.7      Avg. Episode Length: 2.0      Avg. Accuracy: 0.7     \n",
      "[2019-04-06 16:37:18,331] Step: 700020    Avg. Reward: 0.7      Avg. Episode Length: 2.0      Avg. Accuracy: 0.7     \n",
      "[2019-04-06 16:51:20,476] Step: 725022    Avg. Reward: -0.10000000000000009 Avg. Episode Length: 2.0      Avg. Accuracy: 0.5     \n",
      "[2019-04-06 17:04:57,882] Step: 750024    Avg. Reward: -0.1     Avg. Episode Length: 2.0      Avg. Accuracy: 0.5     \n",
      "[2019-04-06 17:19:01,356] Step: 775025    Avg. Reward: 0.7      Avg. Episode Length: 2.0      Avg. Accuracy: 0.7     \n",
      "[2019-04-06 17:33:03,424] Step: 800026    Avg. Reward: 0.7      Avg. Episode Length: 2.0      Avg. Accuracy: 0.7     \n",
      "[2019-04-06 17:47:12,013] Step: 825028    Avg. Reward: 0.7      Avg. Episode Length: 2.0      Avg. Accuracy: 0.7     \n",
      "[2019-04-06 18:01:23,185] Step: 850029    Avg. Reward: -0.49999999999999983 Avg. Episode Length: 2.0      Avg. Accuracy: 0.4     \n",
      "[2019-04-06 18:16:10,175] Step: 875030    Avg. Reward: 0.4      Avg. Episode Length: 1.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-06 18:31:20,381] Step: 900031    Avg. Reward: 0.29999999999999993 Avg. Episode Length: 2.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-06 18:46:14,322] Step: 925032    Avg. Reward: 0.29999999999999993 Avg. Episode Length: 2.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-06 19:00:38,815] Step: 950033    Avg. Reward: -0.8     Avg. Episode Length: 1.0      Avg. Accuracy: 0.3     \n",
      "[2019-04-06 19:15:23,029] Step: 975033    Avg. Reward: 0.29999999999999993 Avg. Episode Length: 2.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-06 19:29:59,421] Step: 1000033   Avg. Reward: -0.10000000000000009 Avg. Episode Length: 2.0      Avg. Accuracy: 0.5     \n",
      "[2019-04-06 19:44:47,965] Step: 1025034   Avg. Reward: -0.9     Avg. Episode Length: 2.0      Avg. Accuracy: 0.3     \n",
      "[2019-04-06 19:59:27,560] Step: 1050034   Avg. Reward: 0.7      Avg. Episode Length: 2.0      Avg. Accuracy: 0.7     \n",
      "[2019-04-06 20:14:11,295] Step: 1075034   Avg. Reward: -0.10000000000000009 Avg. Episode Length: 2.0      Avg. Accuracy: 0.5     \n",
      "[2019-04-06 20:28:53,103] Step: 1100034   Avg. Reward: -0.10000000000000013 Avg. Episode Length: 2.0      Avg. Accuracy: 0.5     \n",
      "[2019-04-06 20:43:17,018] Step: 1125034   Avg. Reward: 0.29999999999999993 Avg. Episode Length: 2.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-06 20:57:28,784] Step: 1150034   Avg. Reward: 0.3      Avg. Episode Length: 2.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-06 21:11:29,913] Step: 1175034   Avg. Reward: 0.7000000000000002 Avg. Episode Length: 2.0      Avg. Accuracy: 0.7     \n",
      "[2019-04-06 21:25:04,743] Step: 1200034   Avg. Reward: 0.7      Avg. Episode Length: 2.0      Avg. Accuracy: 0.7     \n",
      "[2019-04-06 21:38:31,772] Step: 1225034   Avg. Reward: 0.29999999999999993 Avg. Episode Length: 2.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-06 21:52:52,096] Step: 1250034   Avg. Reward: 0.29999999999999993 Avg. Episode Length: 2.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-06 22:07:16,656] Step: 1275035   Avg. Reward: 1.9      Avg. Episode Length: 2.0      Avg. Accuracy: 1.0     \n",
      "[2019-04-06 22:21:11,681] Step: 1300035   Avg. Reward: -0.5000000000000001 Avg. Episode Length: 2.0      Avg. Accuracy: 0.4     \n",
      "[2019-04-06 22:35:16,290] Step: 1325035   Avg. Reward: -0.10000000000000009 Avg. Episode Length: 2.0      Avg. Accuracy: 0.5     \n",
      "[2019-04-06 22:49:25,782] Step: 1350036   Avg. Reward: -0.5     Avg. Episode Length: 2.0      Avg. Accuracy: 0.4     \n",
      "[2019-04-06 23:04:06,910] Step: 1375036   Avg. Reward: -0.10000000000000009 Avg. Episode Length: 2.0      Avg. Accuracy: 0.5     \n",
      "[2019-04-06 23:19:05,092] Step: 1400036   Avg. Reward: 1.1      Avg. Episode Length: 2.0      Avg. Accuracy: 0.8     \n",
      "[2019-04-06 23:33:45,140] Step: 1425037   Avg. Reward: 0.7      Avg. Episode Length: 2.0      Avg. Accuracy: 0.7     \n",
      "[2019-04-06 23:48:20,581] Step: 1450038   Avg. Reward: -0.5     Avg. Episode Length: 2.0      Avg. Accuracy: 0.4     \n",
      "[2019-04-07 00:02:57,660] Step: 1475038   Avg. Reward: -0.10000000000000009 Avg. Episode Length: 2.0      Avg. Accuracy: 0.5     \n",
      "[2019-04-07 00:17:40,155] Step: 1500038   Avg. Reward: 0.6999999999999998 Avg. Episode Length: 2.0      Avg. Accuracy: 0.7     \n",
      "[2019-04-07 00:32:28,389] Step: 1525038   Avg. Reward: 0.29999999999999993 Avg. Episode Length: 2.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-07 00:47:32,763] Step: 1550038   Avg. Reward: 0.29999999999999993 Avg. Episode Length: 2.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-07 01:02:08,686] Step: 1575038   Avg. Reward: 1.1      Avg. Episode Length: 2.0      Avg. Accuracy: 0.8     \n",
      "[2019-04-07 01:16:27,193] Step: 1600038   Avg. Reward: -0.5000000000000001 Avg. Episode Length: 2.0      Avg. Accuracy: 0.4     \n",
      "[2019-04-07 01:30:41,088] Step: 1625039   Avg. Reward: 0.6900000000000001 Avg. Episode Length: 2.1      Avg. Accuracy: 0.7     \n",
      "[2019-04-07 01:44:50,150] Step: 1650041   Avg. Reward: 0.6999999999999998 Avg. Episode Length: 2.0      Avg. Accuracy: 0.7     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-04-07 01:59:36,366] Step: 1675041   Avg. Reward: 0.29999999999999993 Avg. Episode Length: 2.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-07 02:14:36,142] Step: 1700041   Avg. Reward: 1.5000000000000002 Avg. Episode Length: 2.0      Avg. Accuracy: 0.9     \n",
      "[2019-04-07 02:29:18,100] Step: 1725041   Avg. Reward: -0.10000000000000013 Avg. Episode Length: 2.0      Avg. Accuracy: 0.5     \n",
      "[2019-04-07 02:43:56,092] Step: 1750042   Avg. Reward: -0.10000000000000009 Avg. Episode Length: 2.0      Avg. Accuracy: 0.5     \n",
      "[2019-04-07 02:58:30,727] Step: 1775042   Avg. Reward: 0.6999999999999998 Avg. Episode Length: 2.0      Avg. Accuracy: 0.7     \n",
      "[2019-04-07 03:13:41,483] Step: 1800042   Avg. Reward: -0.10000000000000009 Avg. Episode Length: 2.0      Avg. Accuracy: 0.5     \n",
      "[2019-04-07 03:29:03,313] Step: 1825042   Avg. Reward: 0.29999999999999993 Avg. Episode Length: 2.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-07 03:44:09,865] Step: 1850042   Avg. Reward: 0.29999999999999993 Avg. Episode Length: 2.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-07 03:59:16,642] Step: 1875042   Avg. Reward: 1.5000000000000002 Avg. Episode Length: 2.0      Avg. Accuracy: 0.9     \n",
      "[2019-04-07 04:14:20,851] Step: 1900042   Avg. Reward: -0.10000000000000009 Avg. Episode Length: 2.0      Avg. Accuracy: 0.5     \n",
      "[2019-04-07 04:29:33,063] Step: 1925043   Avg. Reward: 0.6999999999999998 Avg. Episode Length: 2.0      Avg. Accuracy: 0.7     \n",
      "[2019-04-07 04:44:27,241] Step: 1950043   Avg. Reward: -0.040000000000000105 Avg. Episode Length: 3.3      Avg. Accuracy: 0.5     \n",
      "[2019-04-07 04:59:02,512] Step: 1975044   Avg. Reward: 1.1      Avg. Episode Length: 2.0      Avg. Accuracy: 0.8     \n",
      "[2019-04-07 05:13:20,987] Step: 2000044   Avg. Reward: 0.29999999999999993 Avg. Episode Length: 2.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-07 05:27:45,066] Step: 2025045   Avg. Reward: -0.11000000000000001 Avg. Episode Length: 2.1      Avg. Accuracy: 0.5     \n",
      "[2019-04-07 05:42:04,882] Step: 2050046   Avg. Reward: 0.29     Avg. Episode Length: 2.1      Avg. Accuracy: 0.6     \n",
      "[2019-04-07 05:56:44,916] Step: 2075046   Avg. Reward: -0.10000000000000009 Avg. Episode Length: 2.0      Avg. Accuracy: 0.5     \n",
      "[2019-04-07 06:11:23,933] Step: 2100047   Avg. Reward: 1.5000000000000002 Avg. Episode Length: 2.0      Avg. Accuracy: 0.9     \n",
      "[2019-04-07 06:25:55,752] Step: 2125048   Avg. Reward: 1.5000000000000002 Avg. Episode Length: 2.0      Avg. Accuracy: 0.9     \n",
      "[2019-04-07 06:40:18,205] Step: 2150048   Avg. Reward: 0.3599999999999999 Avg. Episode Length: 3.3      Avg. Accuracy: 0.6     \n",
      "[2019-04-07 06:54:23,104] Step: 2175048   Avg. Reward: -0.13000000000000006 Avg. Episode Length: 2.3      Avg. Accuracy: 0.5     \n",
      "[2019-04-07 07:08:36,373] Step: 2200048   Avg. Reward: 1.06     Avg. Episode Length: 2.4      Avg. Accuracy: 0.8     \n",
      "[2019-04-07 07:22:36,974] Step: 2225049   Avg. Reward: -0.050000000000000086 Avg. Episode Length: 3.4      Avg. Accuracy: 0.5     \n",
      "[2019-04-07 07:37:04,063] Step: 2250049   Avg. Reward: 1.1      Avg. Episode Length: 2.0      Avg. Accuracy: 0.8     \n",
      "[2019-04-07 07:51:23,950] Step: 2275050   Avg. Reward: 0.29000000000000004 Avg. Episode Length: 2.1      Avg. Accuracy: 0.6     \n",
      "[2019-04-07 08:05:35,434] Step: 2300051   Avg. Reward: 1.05     Avg. Episode Length: 2.5      Avg. Accuracy: 0.8     \n",
      "[2019-04-07 08:19:56,832] Step: 2325051   Avg. Reward: 1.4800000000000002 Avg. Episode Length: 2.2      Avg. Accuracy: 0.9     \n",
      "[2019-04-07 08:33:58,723] Step: 2350051   Avg. Reward: -0.1100000000000001 Avg. Episode Length: 2.1      Avg. Accuracy: 0.5     \n",
      "[2019-04-07 08:47:34,127] Step: 2375051   Avg. Reward: -0.22000000000000003 Avg. Episode Length: 3.2      Avg. Accuracy: 0.5     \n",
      "[2019-04-07 09:01:12,022] Step: 2400051   Avg. Reward: 0.6799999999999999 Avg. Episode Length: 2.2      Avg. Accuracy: 0.7     \n",
      "[2019-04-07 09:14:53,003] Step: 2425052   Avg. Reward: -0.41000000000000014 Avg. Episode Length: 4.9      Avg. Accuracy: 0.4     \n",
      "[2019-04-07 09:28:11,307] Step: 2450052   Avg. Reward: -0.10000000000000009 Avg. Episode Length: 2.0      Avg. Accuracy: 0.5     \n",
      "[2019-04-07 09:41:35,039] Step: 2475052   Avg. Reward: 1.1      Avg. Episode Length: 2.0      Avg. Accuracy: 0.8     \n",
      "[2019-04-07 09:54:53,319] Step: 2500052   Avg. Reward: -0.5000000000000001 Avg. Episode Length: 2.0      Avg. Accuracy: 0.4     \n",
      "[2019-04-07 10:08:40,481] Step: 2525052   Avg. Reward: 1.09     Avg. Episode Length: 4.0      Avg. Accuracy: 0.8     \n",
      "[2019-04-07 10:22:01,315] Step: 2550052   Avg. Reward: 0.6699999999999999 Avg. Episode Length: 2.3      Avg. Accuracy: 0.7     \n",
      "[2019-04-07 10:35:40,990] Step: 2575052   Avg. Reward: -0.15000000000000008 Avg. Episode Length: 2.5      Avg. Accuracy: 0.5     \n",
      "[2019-04-07 10:49:29,664] Step: 2600052   Avg. Reward: 0.2299999999999999 Avg. Episode Length: 2.7      Avg. Accuracy: 0.6     \n",
      "[2019-04-07 11:03:13,905] Step: 2625052   Avg. Reward: 0.7      Avg. Episode Length: 2.0      Avg. Accuracy: 0.7     \n",
      "[2019-04-07 11:16:53,677] Step: 2650053   Avg. Reward: -0.10000000000000005 Avg. Episode Length: 2.0      Avg. Accuracy: 0.5     \n",
      "[2019-04-07 11:30:31,800] Step: 2675053   Avg. Reward: 0.7      Avg. Episode Length: 2.0      Avg. Accuracy: 0.7     \n",
      "[2019-04-07 11:44:27,047] Step: 2700053   Avg. Reward: 0.3299999999999999 Avg. Episode Length: 3.6      Avg. Accuracy: 0.6     \n",
      "[2019-04-07 11:57:58,451] Step: 2725054   Avg. Reward: 1.1      Avg. Episode Length: 2.0      Avg. Accuracy: 0.8     \n",
      "[2019-04-07 12:11:19,968] Step: 2750054   Avg. Reward: 1.09     Avg. Episode Length: 2.1      Avg. Accuracy: 0.8     \n",
      "[2019-04-07 12:24:47,568] Step: 2775055   Avg. Reward: 1.4800000000000002 Avg. Episode Length: 2.2      Avg. Accuracy: 0.9     \n",
      "[2019-04-07 12:38:06,857] Step: 2800055   Avg. Reward: -0.1100000000000001 Avg. Episode Length: 2.1      Avg. Accuracy: 0.5     \n",
      "[2019-04-07 12:51:09,326] Step: 2825055   Avg. Reward: 0.6799999999999999 Avg. Episode Length: 2.2      Avg. Accuracy: 0.7     \n",
      "[2019-04-07 13:04:45,038] Step: 2850055   Avg. Reward: 0.6900000000000001 Avg. Episode Length: 2.1      Avg. Accuracy: 0.7     \n",
      "[2019-04-07 13:18:35,616] Step: 2875055   Avg. Reward: 1.4000000000000001 Avg. Episode Length: 3.0      Avg. Accuracy: 0.9     \n",
      "[2019-04-07 13:32:27,842] Step: 2900055   Avg. Reward: 0.6299999999999999 Avg. Episode Length: 2.7      Avg. Accuracy: 0.7     \n",
      "[2019-04-07 13:46:24,076] Step: 2925056   Avg. Reward: 0.29999999999999993 Avg. Episode Length: 2.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-07 14:00:36,884] Step: 2950060   Avg. Reward: -0.1100000000000001 Avg. Episode Length: 2.1      Avg. Accuracy: 0.5     \n",
      "[2019-04-07 14:14:39,009] Step: 2975060   Avg. Reward: 0.29999999999999993 Avg. Episode Length: 2.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-07 14:28:43,998] Step: 3000060   Avg. Reward: 0.35999999999999993 Avg. Episode Length: 3.3      Avg. Accuracy: 0.6     \n",
      "[2019-04-07 14:42:33,831] Step: 3025060   Avg. Reward: 0.2899999999999999 Avg. Episode Length: 2.1      Avg. Accuracy: 0.6     \n",
      "[2019-04-07 14:55:25,946] Step: 3050060   Avg. Reward: 1.4900000000000002 Avg. Episode Length: 2.1      Avg. Accuracy: 0.9     \n",
      "[2019-04-07 15:08:25,037] Step: 3075061   Avg. Reward: 0.6899999999999998 Avg. Episode Length: 2.1      Avg. Accuracy: 0.7     \n",
      "[2019-04-07 15:21:27,640] Step: 3100061   Avg. Reward: 0.6600000000000001 Avg. Episode Length: 2.4      Avg. Accuracy: 0.7     \n",
      "[2019-04-07 15:34:24,242] Step: 3125061   Avg. Reward: 0.27999999999999986 Avg. Episode Length: 2.2      Avg. Accuracy: 0.6     \n",
      "[2019-04-07 15:47:47,315] Step: 3150061   Avg. Reward: 1.1      Avg. Episode Length: 2.0      Avg. Accuracy: 0.8     \n",
      "[2019-04-07 16:00:49,034] Step: 3175062   Avg. Reward: 0.7000000000000002 Avg. Episode Length: 2.0      Avg. Accuracy: 0.7     \n",
      "[2019-04-07 16:14:01,838] Step: 3200062   Avg. Reward: 1.1      Avg. Episode Length: 2.0      Avg. Accuracy: 0.8     \n",
      "[2019-04-07 16:27:27,193] Step: 3225062   Avg. Reward: -0.12000000000000006 Avg. Episode Length: 2.2      Avg. Accuracy: 0.5     \n",
      "[2019-04-07 16:41:04,410] Step: 3250063   Avg. Reward: -0.10000000000000013 Avg. Episode Length: 2.0      Avg. Accuracy: 0.5     \n",
      "[2019-04-07 16:54:43,730] Step: 3275063   Avg. Reward: -0.5     Avg. Episode Length: 2.0      Avg. Accuracy: 0.4     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-04-07 17:08:18,028] Step: 3300063   Avg. Reward: 0.7      Avg. Episode Length: 2.0      Avg. Accuracy: 0.7     \n",
      "[2019-04-07 17:21:41,226] Step: 3325063   Avg. Reward: 0.6699999999999999 Avg. Episode Length: 2.3      Avg. Accuracy: 0.7     \n",
      "[2019-04-07 17:34:47,045] Step: 3350063   Avg. Reward: 0.29999999999999993 Avg. Episode Length: 2.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-07 17:48:19,995] Step: 3375063   Avg. Reward: -0.17999999999999994 Avg. Episode Length: 2.8      Avg. Accuracy: 0.5     \n",
      "[2019-04-07 18:02:05,598] Step: 3400063   Avg. Reward: 1.0899999999999999 Avg. Episode Length: 2.1      Avg. Accuracy: 0.8     \n",
      "[2019-04-07 18:15:30,471] Step: 3425064   Avg. Reward: 1.09     Avg. Episode Length: 2.1      Avg. Accuracy: 0.8     \n",
      "[2019-04-07 18:29:10,581] Step: 3450065   Avg. Reward: 1.1      Avg. Episode Length: 2.0      Avg. Accuracy: 0.8     \n",
      "[2019-04-07 18:42:51,428] Step: 3475065   Avg. Reward: 1.5000000000000002 Avg. Episode Length: 2.0      Avg. Accuracy: 0.9     \n",
      "[2019-04-07 18:56:38,201] Step: 3500066   Avg. Reward: 0.2999999999999999 Avg. Episode Length: 2.0      Avg. Accuracy: 0.6     \n",
      "[2019-04-07 19:10:29,108] Step: 3525066   Avg. Reward: 1.5000000000000002 Avg. Episode Length: 2.0      Avg. Accuracy: 0.9     \n",
      "[2019-04-07 19:24:16,500] Step: 3550067   Avg. Reward: 0.6999999999999998 Avg. Episode Length: 2.0      Avg. Accuracy: 0.7     \n",
      "[2019-04-07 19:38:04,602] Step: 3575067   Avg. Reward: 0.6900000000000002 Avg. Episode Length: 2.1      Avg. Accuracy: 0.7     \n",
      "[2019-04-07 19:51:55,449] Step: 3600067   Avg. Reward: 0.6799999999999999 Avg. Episode Length: 2.2      Avg. Accuracy: 0.7     \n",
      "[2019-04-07 20:05:42,471] Step: 3625067   Avg. Reward: -0.18000000000000005 Avg. Episode Length: 2.8      Avg. Accuracy: 0.5     \n",
      "[2019-04-07 20:19:19,788] Step: 3650067   Avg. Reward: -0.5099999999999999 Avg. Episode Length: 2.1      Avg. Accuracy: 0.4     \n",
      "[2019-04-07 20:32:49,802] Step: 3675068   Avg. Reward: -0.10000000000000009 Avg. Episode Length: 2.0      Avg. Accuracy: 0.5     \n",
      "[2019-04-07 20:45:53,013] Step: 3700068   Avg. Reward: 0.68     Avg. Episode Length: 2.2      Avg. Accuracy: 0.7     \n",
      "[2019-04-07 20:58:47,907] Step: 3725068   Avg. Reward: 0.29     Avg. Episode Length: 2.1      Avg. Accuracy: 0.6     \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # BLAS setup\n",
    "    os.environ['OMP_NUM_THREADS'] = '2'\n",
    "    os.environ['MKL_NUM_THREADS'] = '2'\n",
    "    \n",
    "    # Setup\n",
    "    args = parser.parse_args(args=[])\n",
    "    # Creating directories.\n",
    "    save_dir = os.path.join('.', args.name)  \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)  \n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    T = Counter()  # Global shared counter\n",
    "    \n",
    "    shared_model = ActorCritic(N_DR_ELEMENTS+N_DR_ELEMENTS, N_ACTIONS, args.hidden_size)\n",
    "    shared_model.share_memory()\n",
    "    \n",
    "    if args.pretrain_model_available:\n",
    "        # Load pretrained weights\n",
    "        shared_model.load_state_dict(torch.load('model.pth'))\n",
    "        \n",
    "    # Create average network\n",
    "    shared_average_model = ActorCritic(N_DR_ELEMENTS+N_DR_ELEMENTS, N_ACTIONS, args.hidden_size)\n",
    "    shared_average_model.load_state_dict(shared_model.state_dict())\n",
    "    shared_average_model.share_memory()\n",
    "    \n",
    "    for param in shared_average_model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # Create optimiser for shared network parameters with shared statistics\n",
    "    optimiser = SharedRMSprop(shared_model.parameters(), lr=args.lr, alpha=args.rmsprop_decay)\n",
    "    optimiser.share_memory()\n",
    "    \n",
    "    fields = ['t', 'rewards', 'avg_steps', 'accuracy', 'time']\n",
    "    with open(os.path.join(save_dir, 'test_results.csv'), 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(fields)\n",
    "    \n",
    "    processes = []\n",
    "    # Start validation agent\n",
    "    p = threading.Thread(target=test, args=(0, args, T, shared_model))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "\n",
    "    # Start training agents\n",
    "    for rank in range(1, args.num_processes + 1):\n",
    "        t = threading.Thread(target=train, args=(rank, args, T, shared_model, shared_average_model, optimiser))\n",
    "        t.start()\n",
    "        #p = mp.Process(target=train, args=(rank, args, T, shared_model, shared_average_model, optimiser))\n",
    "        #p.start()\n",
    "        print('Process ' + str(rank) + ' started')\n",
    "        processes.append(t)\n",
    "    \n",
    "    for p in processes:\n",
    "        print(p.is_alive())\n",
    "    # Clean up\n",
    "    for p in processes:\n",
    "        p.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.pretrain_model_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
